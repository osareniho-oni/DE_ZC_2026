# ============================================================================
# WORKFLOW METADATA
# ============================================================================
# Loads FHV parquet data from GCS into BigQuery with explicit schema handling
# Prevents schema drift issues by using predefined table schemas and MERGE operations

id: load_fhv_to_bq
namespace: zoomcamp.subflows

description: |
  Loads FHV parquet data from GCS into BigQuery using explicit schemas.
  Handles FHV trip data with proper time partitioning and deduplication.
  Prevents schema drift by creating tables with fixed schemas before loading data.

# ============================================================================
# USER INPUTS
# ============================================================================
# Parameters passed from the parent workflow

inputs:
  - id: year
    type: STRING
    description: Year of the data
    
  - id: month
    type: STRING
    description: Month in 2-digit format
    
  - id: gcs_object
    type: STRING
    description: GCS object path relative to bucket

# ============================================================================
# WORKFLOW VARIABLES
# ============================================================================
# Defines reusable variables for GCP project, dataset, and table references

variables:
  project: dlt-bigquery-484316
  dataset: wk1_tf_dataset
  bucket: wk1-tf-bucket

# ============================================================================
# TASKS
# ============================================================================
# The workflow tasks execute in sequence to load and merge data into BigQuery

tasks:
  # --------------------------------------------------------------------------
  # TASK 1: Create FHV Main Table
  # --------------------------------------------------------------------------
  # Creates the partitioned main table with explicit schema if it doesn't exist
  # This prevents schema drift issues across different months
  
  - id: create_fhv_main_table
    type: io.kestra.plugin.gcp.bigquery.Query
    projectId: "{{ vars.project }}"
    sql: |
      CREATE TABLE IF NOT EXISTS `{{ vars.project }}.{{ vars.dataset }}.fhv_tripdata`
      (
          unique_row_id BYTES OPTIONS (description = 'A unique identifier for the trip, generated by hashing key trip attributes'),
          filename STRING OPTIONS (description = 'The source filename from which the trip data was loaded'),
          dispatching_base_num STRING OPTIONS (description = 'The TLC Base License Number of the base that dispatched the trip'),
          pickup_datetime TIMESTAMP OPTIONS (description = 'The date and time of the trip pick-up'),
          dropOff_datetime TIMESTAMP OPTIONS (description = 'The date and time of the trip dropoff'),
          PUlocationID STRING OPTIONS (description = 'TLC Taxi Zone in which the trip began'),
          DOlocationID STRING OPTIONS (description = 'TLC Taxi Zone in which the trip ended'),
          SR_Flag STRING OPTIONS (description = 'Indicates if the trip was a part of a shared ride chain. 1 = shared ride, null = non-shared ride'),
          Affiliated_base_number STRING OPTIONS (description = 'Base number of the base with which the vehicle is affiliated')
      )
      PARTITION BY DATE(pickup_datetime)
      CLUSTER BY dispatching_base_num, PUlocationID, DOlocationID
      OPTIONS (
          description = 'FHV (For-Hire Vehicle) trip data with partitioning by pickup date and clustering by base and locations'
      );

  # --------------------------------------------------------------------------
  # TASK 2: Create FHV External Table
  # --------------------------------------------------------------------------
  # Creates an external table pointing to the parquet file in GCS
  # This allows BigQuery to query the parquet directly without loading it first
  
  - id: create_fhv_external_table
    type: io.kestra.plugin.gcp.bigquery.Query
    projectId: "{{ vars.project }}"
    sql: |
      CREATE OR REPLACE EXTERNAL TABLE `{{ vars.project }}.{{ vars.dataset }}.fhv_tripdata_{{ inputs.year }}_{{ inputs.month }}_ext`
      OPTIONS (
          format = 'PARQUET',
          uris = ['gs://{{ vars.bucket }}/{{ inputs.gcs_object }}']
      );

  # --------------------------------------------------------------------------
  # TASK 3: Create FHV Temporary Table with Unique IDs
  # --------------------------------------------------------------------------
  # Creates a temporary table from the external table
  # Adds unique_row_id (MD5 hash) and filename for deduplication
  # Handles missing columns gracefully with COALESCE
  
  - id: create_fhv_temp_table
    type: io.kestra.plugin.gcp.bigquery.Query
    projectId: "{{ vars.project }}"
    sql: |
      CREATE OR REPLACE TABLE `{{ vars.project }}.{{ vars.dataset }}.fhv_tripdata_{{ inputs.year }}_{{ inputs.month }}`
      AS
      SELECT
        MD5(CONCAT(
          COALESCE(CAST(dispatching_base_num AS STRING), ""),
          COALESCE(CAST(pickup_datetime AS STRING), ""),
          COALESCE(CAST(dropOff_datetime AS STRING), ""),
          COALESCE(CAST(PUlocationID AS STRING), ""),
          COALESCE(CAST(DOlocationID AS STRING), "")
        )) AS unique_row_id,
        "{{ inputs.gcs_object }}" AS filename,
        CAST(dispatching_base_num AS STRING) AS dispatching_base_num,
        pickup_datetime,
        dropOff_datetime,
        CAST(PUlocationID AS STRING) AS PUlocationID,
        CAST(DOlocationID AS STRING) AS DOlocationID,
        CAST(SR_Flag AS STRING) AS SR_Flag,
        CAST(Affiliated_base_number AS STRING) AS Affiliated_base_number
      FROM `{{ vars.project }}.{{ vars.dataset }}.fhv_tripdata_{{ inputs.year }}_{{ inputs.month }}_ext`;

  # --------------------------------------------------------------------------
  # TASK 4: Merge FHV Data into Main Table
  # --------------------------------------------------------------------------
  # Performs a MERGE operation to insert only new records (no duplicates)
  # Uses unique_row_id to identify existing records
  
  - id: merge_fhv_data
    type: io.kestra.plugin.gcp.bigquery.Query
    projectId: "{{ vars.project }}"
    sql: |
      MERGE INTO `{{ vars.project }}.{{ vars.dataset }}.fhv_tripdata` T
      USING `{{ vars.project }}.{{ vars.dataset }}.fhv_tripdata_{{ inputs.year }}_{{ inputs.month }}` S
      ON T.unique_row_id = S.unique_row_id
      WHEN NOT MATCHED THEN
        INSERT (unique_row_id, filename, dispatching_base_num, pickup_datetime, dropOff_datetime, PUlocationID, DOlocationID, SR_Flag, Affiliated_base_number)
        VALUES (S.unique_row_id, S.filename, S.dispatching_base_num, S.pickup_datetime, S.dropOff_datetime, S.PUlocationID, S.DOlocationID, S.SR_Flag, S.Affiliated_base_number);

  # --------------------------------------------------------------------------
  # TASK 5: Cleanup FHV Temporary Tables
  # --------------------------------------------------------------------------
  # Drops the temporary and external tables to save storage costs
  
  - id: cleanup_fhv_tables
    type: io.kestra.plugin.gcp.bigquery.Query
    projectId: "{{ vars.project }}"
    sql: |
      DROP TABLE IF EXISTS `{{ vars.project }}.{{ vars.dataset }}.fhv_tripdata_{{ inputs.year }}_{{ inputs.month }}`;
      DROP TABLE IF EXISTS `{{ vars.project }}.{{ vars.dataset }}.fhv_tripdata_{{ inputs.year }}_{{ inputs.month }}_ext`;

# ============================================================================
# PLUGIN DEFAULTS
# ============================================================================
# Sets default configuration for all GCP plugin tasks

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      projectId: "{{ vars.project }}"